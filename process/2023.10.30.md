标题：XXXNet（后面取个牛逼的名字）：基于文本对抗和多模型融合的中文文本非法评论检测工具



目前存在的不足：非法评论上的类别分布不均，导致一些只有少量数据的非法评论无法被正确识别
解决方法:对非法评论上的样本进行分类，对少量的非法评论数据进行增强，最终生成一个非法评论检测工具


方法流程：
1.文本聚类：对非法评论上的样本进行分类

2.文本对抗数据增强：对少量的非法评论数据进行增强

3.文本分类：

Chinese-BERT：针对中文文本的BERT模型，因此适合用于处理中文非法评论数据。它可以用来提取文本的上下文相关表示，从而捕捉评论中的复杂语义和情感信息。

Attention：在处理非法评论时，注意力机制可以帮助模型关注评论中的重要部分，增强模型对文本的重要性的感知，有助于更好地区分合法和非法评论。

TextCNN：通过卷积层可以有效地捕获文本中的局部特征，这对于理解网络用语中的特殊表达方式（如缩写、俚语、网络新词汇）非常有帮助。卷积操作能够捕捉短语级别的特征，使模型能够检测出这些特殊表达。

LSTM：在处理评论时，尤其是较长的评论，LSTM可以帮助模型捕获文本中的时序信息和上下文关系。可以去探索用户名，评论的主题和评论的内容之间的相关性。

4.检测：验证模型的实用性




相关工作（还缺近2年的）：

 BERT (Devlin et al., 2018) 建立在 Transformer 架构 (Vaswani et al., 2017) 之上，以 Masked Language Model (MLM) 和 Next Sentence 的方式在大规模无标签文本语料库上进行预训练预测（NSP）。
然而，RoBERTa（Liu et al., 2019b）建议删除 NSP 预训练任务，因为它已被证明对提高下游性能没有任何好处。还有其他BERT变体和GPT系列等模型，它们在大规模无监督预训练方面进行了探索，应用于文本生成任务，例如机器翻译、文本摘要和对话生成。

与英语不同，汉语在表达语义、语法、词汇和发音方面有其独特的特点。
Li等人。 （2019b）提出使用汉字作为基本单位，而不是英语中使用的单词或子词（Wu et al., 2016; Sennrich et al., 2016）

预训练模型最初是为英语设计的，因此针对当前大规模预训练中缺少中文的情况。
因此，应该设计一种适合中文汉语特点的模型是必要的。

Zhang等人。 (2020) 开发了一种大规模的中文预训练语言模型 CPM。它在 100GB 中文数据上进行预训练，具有与“GPT3 2.7B”相当的 2.6B 参数（Brown 等人，2020）。

ChineseBERT-bert将基于字形的信息和基于拼音的信息融入到预训练模型中去。
（ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information 2021 ACL）

但是上述预训练模型是基于标准的汉语进行训练的，然而对于一些非法的评论，会出现很多网络用语和网络梗，句子缺乏连贯性，在句子理解上会造成一定的困难。
